# -*- coding: utf-8 -*-
"""Leadership

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17VpqHg__-5DsDp29C8LptZ0SwCwqnKXy
"""

!pip install faker

from faker import Faker
import random
import pandas as pd

# Initialize Faker
fake = Faker()

# Define the number of entries
num_entries = 100

# Generate the dataset
data = {
    'Decision-Making Efficiency (days)': [random.randint(1, 60) for _ in range(num_entries)],
    'Team Engagement (score)': [random.uniform(1, 5) for _ in range(num_entries)],
    'Communication Effectiveness (score)': [random.uniform(1, 5) for _ in range(num_entries)],
    'Cultural Competency Metrics (score)': [random.uniform(1, 5) for _ in range(num_entries)],
    'Innovation Index (score)': [random.uniform(1, 5) for _ in range(num_entries)],
    'Workforce Analytics (score)': [random.uniform(1, 5) for _ in range(num_entries)],
    'Leadership Development Tracking (score)': [random.uniform(1, 5) for _ in range(num_entries)],
    'Employee Well-being Index (score)': [random.uniform(1, 5) for _ in range(num_entries)],
    'Collaboration Network Analysis (score)': [random.uniform(1, 5) for _ in range(num_entries)],
    'Skill Gap Analysis (score)': [random.uniform(1, 5) for _ in range(num_entries)],
    'Feedback and Recognition System (score)': [random.uniform(1, 5) for _ in range(num_entries)],
    'AI-Powered Decision Support (score)': [random.uniform(1, 5) for _ in range(num_entries)],
    'Global Trend Analysis (score)': [random.uniform(1, 5) for _ in range(num_entries)]
}

# Create a DataFrame
df = pd.DataFrame(data)

# Show a snippet of the dataset
df.head()

# Adding a column for random names
data['Team Leader'] = [fake.name() for _ in range(num_entries)]

# Create a new DataFrame with the added names
df_with_names = pd.DataFrame(data)

# Show a snippet of the dataset with names
df_with_names.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Statistical overview
statistical_overview = df_with_names.describe()

# Plotting distributions of some key metrics
plt.figure(figsize=(15, 10))

# Team Engagement
plt.subplot(2, 2, 1)
sns.histplot(df_with_names['Team Engagement (score)'], kde=True)
plt.title('Distribution of Team Engagement Scores')

# Communication Effectiveness
plt.subplot(2, 2, 2)
sns.histplot(df_with_names['Communication Effectiveness (score)'], kde=True)
plt.title('Distribution of Communication Effectiveness Scores')

# Leadership Development
plt.subplot(2, 2, 3)
sns.histplot(df_with_names['Leadership Development Tracking (score)'], kde=True)
plt.title('Distribution of Leadership Development Scores')

# Employee Well-being Index
plt.subplot(2, 2, 4)
sns.histplot(df_with_names['Employee Well-being Index (score)'], kde=True)
plt.title('Distribution of Employee Well-being Index Scores')

plt.tight_layout()
plt.show()

statistical_overview

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Creating the target variable (1 for effective leader, 0 for ineffective)
# Defining a leader as effective if the average score across all metrics is above 3.5
df_with_names['Effective Leader'] = df_with_names.iloc[:, 0:13].mean(axis=1) > 3.5

# Splitting the dataset into features (X) and target (y)
X = df_with_names.iloc[:, 0:13]  # all score columns
y = df_with_names['Effective Leader']

# Splitting the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Standardizing the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Training the Logistic Regression model
model = LogisticRegression()
model.fit(X_train_scaled, y_train)

# Predicting the test set results
y_pred = model.predict(X_test_scaled)

# Evaluating the model
accuracy = model.score(X_test_scaled, y_test)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

accuracy, conf_matrix, class_report

# Generating a random sample for prediction
random_sample = {feature: random.uniform(1, 5) if 'score' in feature else random.randint(1, 60)
                 for feature in df_with_names.columns[:-2]}

# Creating a DataFrame for the sample
sample_df = pd.DataFrame([random_sample])

# Standardizing the sample
sample_scaled = scaler.transform(sample_df)

# Making a prediction using the model
prediction = model.predict(sample_scaled)
prediction_proba = model.predict_proba(sample_scaled)

# Preparing the output
prediction_result = 'Effective' if prediction[0] else 'Ineffective'
probability_effective = prediction_proba[0][1]

sample_df, prediction_result, probability_effective